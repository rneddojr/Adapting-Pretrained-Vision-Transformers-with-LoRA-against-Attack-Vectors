{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b989893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.optim as optimizer\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66848500",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224), # ViT requires 224x224 input\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert to 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalize image using mean and std deviation\n",
    "])\n",
    "\n",
    "train_ds = datasets.FashionMNIST(root=\"./fashion_data\", train=True, download=True, transform=transform)\n",
    "test_ds = datasets.FashionMNIST(root=\"./fashion_data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=264, shuffle=False)\n",
    "\n",
    "num_classes = len(train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b6621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks\n",
    "train_loader.dataset.data.shape\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c8824",
   "metadata": {},
   "source": [
    "## Load pretrained vit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a957407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "there are 768 number of output parameters in the last layer. So unfreeze it to learn the 10 in fashion mnist\n",
      "Trainable parameters in the model:\n",
      "  encoder.layers.encoder_layer_10.ln_1.weight\n",
      "  encoder.layers.encoder_layer_10.ln_1.bias\n",
      "  encoder.layers.encoder_layer_10.self_attention.in_proj_weight\n",
      "  encoder.layers.encoder_layer_10.self_attention.in_proj_bias\n",
      "  encoder.layers.encoder_layer_10.self_attention.out_proj.weight\n",
      "  encoder.layers.encoder_layer_10.self_attention.out_proj.bias\n",
      "  encoder.layers.encoder_layer_10.ln_2.weight\n",
      "  encoder.layers.encoder_layer_10.ln_2.bias\n",
      "  encoder.layers.encoder_layer_10.mlp.0.weight\n",
      "  encoder.layers.encoder_layer_10.mlp.0.bias\n",
      "  encoder.layers.encoder_layer_10.mlp.3.weight\n",
      "  encoder.layers.encoder_layer_10.mlp.3.bias\n",
      "  encoder.layers.encoder_layer_11.ln_1.weight\n",
      "  encoder.layers.encoder_layer_11.ln_1.bias\n",
      "  encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
      "  encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
      "  encoder.layers.encoder_layer_11.self_attention.out_proj.weight\n",
      "  encoder.layers.encoder_layer_11.self_attention.out_proj.bias\n",
      "  encoder.layers.encoder_layer_11.ln_2.weight\n",
      "  encoder.layers.encoder_layer_11.ln_2.bias\n",
      "  encoder.layers.encoder_layer_11.mlp.0.weight\n",
      "  encoder.layers.encoder_layer_11.mlp.0.bias\n",
      "  encoder.layers.encoder_layer_11.mlp.3.weight\n",
      "  encoder.layers.encoder_layer_11.mlp.3.bias\n",
      "  heads.head.0.weight\n",
      "  heads.head.0.bias\n",
      "  heads.head.3.weight\n",
      "  heads.head.3.bias\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(\"Using device:\", device)\n",
    "vit_model = models.vit_b_16(weights='DEFAULT')\n",
    "vit_model.to(device)\n",
    "vit_model.eval()  # freeze backbone\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_features = vit_model.heads.head.in_features\n",
    "print(f'there are {n_features} number of input parameters in the last layer')\n",
    "\n",
    "vit_model.heads.head = nn.Sequential(nn.Linear(n_features, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes)) # Replace classification head:\n",
    "for i in [-1,-2]: # Train last 2 layers besides the final classification layer.\n",
    "    for param in vit_model.encoder.layers[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# # Safety check\n",
    "# print(\"Trainable parameters in the model:\")\n",
    "# for name, param in vit_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bf716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5142b904",
   "metadata": {},
   "source": [
    "## Simple BiLoRA layer Class\n",
    "\n",
    "$$W_t = W_0 + \\Delta W_t$$\n",
    "where $$\\Delta W_t = UB_tV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d012be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLoRALinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_frq=100, alpha=100.0, n_tasks=3):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_frq = n_frq\n",
    "        self.alpha = alpha\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        # Base weight (frozen backbone)\n",
    "        self.base_weight = nn.Parameter(torch.randn(out_dim, in_dim) * 0.02)\n",
    "        self.base_bias = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        # Per-task frequency coefficients\n",
    "        self.coef = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(n_frq), requires_grad=True)\n",
    "            for _ in range(n_tasks)\n",
    "        ])\n",
    "        self.indices = [self._select_positions(t) for t in range(n_tasks)]\n",
    "\n",
    "    def _select_positions(self, t, seed=777):\n",
    "        torch.manual_seed(seed + t)\n",
    "        idx = torch.randperm(self.out_dim * self.in_dim)[:self.n_frq]\n",
    "        return torch.stack([idx // self.in_dim, idx % self.in_dim])\n",
    "\n",
    "    def _get_delta_w(self, task):\n",
    "        # frequency-domain delta\n",
    "        Freq = torch.zeros(self.out_dim, self.in_dim, device=self.base_weight.device)\n",
    "        Freq[self.indices[task][0], self.indices[task][1]] = self.coef[task]\n",
    "        delta_w = torch.fft.ifft2(Freq).real * self.alpha\n",
    "        return delta_w\n",
    "\n",
    "    def forward(self, x, task_id=0):\n",
    "        delta_w = self._get_delta_w(task_id)\n",
    "        w_eff = self.base_weight + delta_w\n",
    "        return F.linear(x, w_eff, self.base_bias)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
